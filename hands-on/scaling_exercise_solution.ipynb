{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On: Strong and Weak Scaling with SLURM\n",
    "\n",
    "**Objective:** Apply Amdahl's and Gustafson's Laws in practice by measuring strong and weak scaling on an HPC cluster.\n",
    "\n",
    "In this exercise, you will:\n",
    "1. Compile an OpenMP-parallelized Julia set computation\n",
    "2. Submit SLURM job arrays to measure strong and weak scaling\n",
    "3. Analyze the results and compare them to theoretical predictions\n",
    "\n",
    "---\n",
    "\n",
    "**Acknowledgment:** This exercise is inspired by the tutorial [\"Scalability: Strong and Weak Scaling\"](https://www.kth.se/blogs/pdc/2018/11/scalability-strong-and-weak-scaling/) from KTH PDC, which references the Julia set OpenMP code from [John Burkardt (FSU)](https://people.sc.fsu.edu/~jburkardt/c_src/julia_set_openmp/julia_set_openmp.c).\n",
    "\n",
    "---\n",
    "\n",
    "### Using Jupyter Notebooks\n",
    "\n",
    "If you're new to Jupyter notebooks, here are the essentials:\n",
    "- **Shift+Enter**: Execute the current cell and move to the next one\n",
    "- **Ctrl+Enter**: Execute the current cell and stay on it\n",
    "- Cells can contain **code** (Python) or **Markdown** (formatted text)\n",
    "- Execute cells in order from top to bottom\n",
    "- If something goes wrong, you can restart the kernel via *Kernel → Restart*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theory Recap\n",
    "\n",
    "### Strong Scaling (Amdahl's Law)\n",
    "\n",
    "**Question:** How much faster can we solve a *fixed-size* problem by adding more processors?\n",
    "\n",
    "$$S(N) = \\frac{1}{(1-p) + \\frac{p}{N}}$$\n",
    "\n",
    "Where:\n",
    "- $S(N)$ = speedup with $N$ processors\n",
    "- $p$ = parallelizable fraction of the code\n",
    "- $(1-p)$ = serial fraction\n",
    "\n",
    "**Key insight:** Even with infinite processors, speedup is limited by the serial fraction: $S_{max} = \\frac{1}{1-p}$\n",
    "\n",
    "### Weak Scaling (Gustafson's Law)\n",
    "\n",
    "**Question:** How much larger a problem can we solve in the *same time* by adding more processors?\n",
    "\n",
    "$$S(N) = N - s(N-1) = s + p \\cdot N$$\n",
    "\n",
    "Where:\n",
    "- $s$ = serial fraction\n",
    "- $p = 1 - s$ = parallel fraction\n",
    "\n",
    "**Key insight:** Speedup grows linearly with $N$ — more optimistic for large-scale computing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Julia Set Algorithm\n",
    "\n",
    "The [Julia set](https://en.wikipedia.org/wiki/Julia_set) is a fractal defined by iterating the complex function:\n",
    "\n",
    "$$Z_{k+1} = Z_k^2 + C$$\n",
    "\n",
    "For each pixel (mapped to a complex number $Z_0$), we iterate until either:\n",
    "- The magnitude $|Z_k|$ exceeds a threshold (point escapes → not in set)\n",
    "- We reach the maximum iterations (point stays bounded → in set)\n",
    "\n",
    "The iteration count determines the pixel color, producing beautiful fractal images.\n",
    "\n",
    "**Why Julia set for scaling tests?**\n",
    "- Each pixel is computed independently → embarrassingly parallel\n",
    "- Computation time scales linearly with image size\n",
    "- Easy to vary problem size (just change resolution)\n",
    "\n",
    "We use $C = -0.8 + 0.156i$ which produces a visually interesting fractal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup and Configuration\n",
    "\n",
    "First, let's configure matplotlib and set experiment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib configuration for interactive figures\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 8)\n",
    "matplotlib.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT PARAMETERS - Adjust these as needed\n",
    "# =============================================================================\n",
    "\n",
    "# Artificial serial time (in milliseconds) to add to each run.\n",
    "# This makes the effect of Amdahl's Law more visible in the plots.\n",
    "# Set to 0 for pure Julia set computation timing.\n",
    "ARTIFICIAL_SERIAL_MS = 100  # Try different values: 0, 50, 100, 200\n",
    "\n",
    "# Problem sizes\n",
    "STRONG_SCALING_SIZE = 4000  # Fixed size for strong scaling (width = height)\n",
    "WEAK_SCALING_BASE = 1000    # Base size for weak scaling (1 thread)\n",
    "\n",
    "print(f\"Artificial serial time: {ARTIFICIAL_SERIAL_MS} ms\")\n",
    "print(f\"Strong scaling: {STRONG_SCALING_SIZE} x {STRONG_SCALING_SIZE} pixels\")\n",
    "print(f\"Weak scaling base: {WEAK_SCALING_BASE} x {WEAK_SCALING_BASE} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directories for logs and output\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "print(\"Directories created:\")\n",
    "print(\"  - logs/   (SLURM output files)\")\n",
    "print(\"  - output/ (Julia set images)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The C Code\n",
    "\n",
    "The following cell writes the OpenMP-parallelized Julia set code to `julia_set.c`.\n",
    "\n",
    "Key features:\n",
    "- Uses `#pragma omp parallel for` to distribute pixel computation across threads\n",
    "- Uses `omp_get_wtime()` for precise timing\n",
    "- Accepts an optional **artificial serial delay** to demonstrate Amdahl's Law\n",
    "- Outputs timing info to stdout (captured by SLURM)\n",
    "- Writes image as binary file (easy to read in Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile julia_set.c\n",
    "/*\n",
    " * Julia Set computation with OpenMP parallelization\n",
    " * \n",
    " * Inspired by the KTH PDC scaling tutorial and code from John Burkardt (FSU):\n",
    " * https://www.kth.se/blogs/pdc/2018/11/scalability-strong-and-weak-scaling/\n",
    " * https://people.sc.fsu.edu/~jburkardt/c_src/julia_set_openmp/julia_set_openmp.c\n",
    " *\n",
    " * Usage: ./julia_set <width> <height> <output_file> [serial_ms]\n",
    " *   width, height : image dimensions\n",
    " *   output_file   : binary output file\n",
    " *   serial_ms     : optional artificial serial delay in milliseconds (default: 0)\n",
    " *\n",
    " * Thread count is controlled via OMP_NUM_THREADS environment variable.\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <stdint.h>\n",
    "#include <time.h>\n",
    "#include <omp.h>\n",
    "\n",
    "/* Julia set parameters */\n",
    "#define C_REAL -0.8\n",
    "#define C_IMAG  0.156\n",
    "#define MAX_ITER 200\n",
    "#define ESCAPE_RADIUS 1000.0\n",
    "\n",
    "/* Domain boundaries */\n",
    "#define X_MIN -1.5\n",
    "#define X_MAX  1.5\n",
    "#define Y_MIN -1.5\n",
    "#define Y_MAX  1.5\n",
    "\n",
    "/*\n",
    " * Artificial serial delay using busy-wait.\n",
    " * This simulates a non-parallelizable portion of the code.\n",
    " */\n",
    "void artificial_serial_delay(double milliseconds) {\n",
    "    if (milliseconds <= 0) return;\n",
    "    \n",
    "    double seconds = milliseconds / 1000.0;\n",
    "    double start = omp_get_wtime();\n",
    "    \n",
    "    /* Busy-wait loop */\n",
    "    while (omp_get_wtime() - start < seconds) {\n",
    "        /* Do nothing - just wait */\n",
    "    }\n",
    "}\n",
    "\n",
    "/*\n",
    " * Compute the iteration count for a single point.\n",
    " * Returns the number of iterations before escape, or MAX_ITER if bounded.\n",
    " */\n",
    "int julia_iterate(double x0, double y0) {\n",
    "    double x = x0;\n",
    "    double y = y0;\n",
    "    int iter;\n",
    "    \n",
    "    for (iter = 0; iter < MAX_ITER; iter++) {\n",
    "        double x_new = x * x - y * y + C_REAL;\n",
    "        double y_new = 2.0 * x * y + C_IMAG;\n",
    "        x = x_new;\n",
    "        y = y_new;\n",
    "        \n",
    "        if (x * x + y * y > ESCAPE_RADIUS * ESCAPE_RADIUS) {\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return iter;\n",
    "}\n",
    "\n",
    "/*\n",
    " * Compute the Julia set for the entire image.\n",
    " * This is where OpenMP parallelization happens.\n",
    " */\n",
    "void compute_julia_set(unsigned char *image, int width, int height) {\n",
    "    double x_scale = (X_MAX - X_MIN) / (double)(width - 1);\n",
    "    double y_scale = (Y_MAX - Y_MIN) / (double)(height - 1);\n",
    "    \n",
    "    #pragma omp parallel for schedule(dynamic)\n",
    "    for (int j = 0; j < height; j++) {\n",
    "        double y0 = Y_MAX - j * y_scale;  /* Flip y for image coordinates */\n",
    "        \n",
    "        for (int i = 0; i < width; i++) {\n",
    "            double x0 = X_MIN + i * x_scale;\n",
    "            int iter = julia_iterate(x0, y0);\n",
    "            \n",
    "            /* Map iteration count to grayscale (0-255) */\n",
    "            image[j * width + i] = (unsigned char)(iter % 256);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "/*\n",
    " * Write the image as a binary file.\n",
    " * Format: [width (4 bytes)] [height (4 bytes)] [pixels (width*height bytes)]\n",
    " */\n",
    "int write_binary_image(const char *filename, unsigned char *image, int width, int height) {\n",
    "    FILE *fp = fopen(filename, \"wb\");\n",
    "    if (fp == NULL) {\n",
    "        fprintf(stderr, \"Error: Cannot open file %s for writing\\n\", filename);\n",
    "        return -1;\n",
    "    }\n",
    "    \n",
    "    /* Write header: width and height as 32-bit integers */\n",
    "    int32_t w = width;\n",
    "    int32_t h = height;\n",
    "    fwrite(&w, sizeof(int32_t), 1, fp);\n",
    "    fwrite(&h, sizeof(int32_t), 1, fp);\n",
    "    \n",
    "    /* Write pixel data */\n",
    "    fwrite(image, sizeof(unsigned char), width * height, fp);\n",
    "    \n",
    "    fclose(fp);\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "    if (argc < 4) {\n",
    "        printf(\"Usage: %s <width> <height> <output_file> [serial_ms]\\n\", argv[0]);\n",
    "        printf(\"  serial_ms: artificial serial delay in milliseconds (default: 0)\\n\");\n",
    "        printf(\"  Thread count is controlled via OMP_NUM_THREADS.\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    int width = atoi(argv[1]);\n",
    "    int height = atoi(argv[2]);\n",
    "    const char *output_file = argv[3];\n",
    "    double serial_ms = (argc > 4) ? atof(argv[4]) : 0.0;\n",
    "    \n",
    "    /* Get thread count */\n",
    "    int num_threads;\n",
    "    #pragma omp parallel\n",
    "    {\n",
    "        #pragma omp single\n",
    "        num_threads = omp_get_num_threads();\n",
    "    }\n",
    "    \n",
    "    /* Allocate image buffer */\n",
    "    unsigned char *image = (unsigned char *)malloc(width * height * sizeof(unsigned char));\n",
    "    if (image == NULL) {\n",
    "        fprintf(stderr, \"Error: Cannot allocate memory for %dx%d image\\n\", width, height);\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    /* Start timing */\n",
    "    double start_time = omp_get_wtime();\n",
    "    \n",
    "    /* Artificial serial portion (not parallelizable) */\n",
    "    artificial_serial_delay(serial_ms);\n",
    "    \n",
    "    /* Compute Julia set (parallel portion) */\n",
    "    compute_julia_set(image, width, height);\n",
    "    \n",
    "    /* End timing */\n",
    "    double end_time = omp_get_wtime();\n",
    "    double elapsed = end_time - start_time;\n",
    "    \n",
    "    /* Output timing information (parsed by Python later) */\n",
    "    printf(\"Threads: %d\\n\", num_threads);\n",
    "    printf(\"Width: %d\\n\", width);\n",
    "    printf(\"Height: %d\\n\", height);\n",
    "    printf(\"SerialMS: %.1f\\n\", serial_ms);\n",
    "    printf(\"Time: %.6f\\n\", elapsed);\n",
    "    \n",
    "    /* Write output image */\n",
    "    if (write_binary_image(output_file, image, width, height) != 0) {\n",
    "        free(image);\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    printf(\"Output: %s\\n\", output_file);\n",
    "    \n",
    "    free(image);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compilation\n",
    "\n",
    "Compile the code with OpenMP support. Run this command in your terminal:\n",
    "\n",
    "```bash\n",
    "gcc -O3 -fopenmp -o julia_set julia_set.c\n",
    "```\n",
    "\n",
    "Flags:\n",
    "- `-O3`: Aggressive optimization\n",
    "- `-fopenmp`: Enable OpenMP support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile from within the notebook:\n",
    "!gcc -O3 -fopenmp -o julia_set julia_set.c && echo \"Compilation successful!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Local Test (Optional)\n",
    "\n",
    "Before submitting to SLURM, let's verify the code works locally with a small test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick local test with 2 threads, small image, and artificial serial delay\n",
    "!OMP_NUM_THREADS=2 ./julia_set 500 500 output/test_local.bin {ARTIFICIAL_SERIAL_MS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the test output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_julia_image(filename):\n",
    "    \"\"\"Read a binary Julia set image file.\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Read header\n",
    "        width = np.frombuffer(f.read(4), dtype=np.int32)[0]\n",
    "        height = np.frombuffer(f.read(4), dtype=np.int32)[0]\n",
    "        # Read pixel data\n",
    "        pixels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        return pixels.reshape(height, width), width, height\n",
    "\n",
    "# Display the test image\n",
    "img, w, h = read_julia_image('output/test_local.bin')\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img, cmap='hot')\n",
    "plt.colorbar(label='Iterations')\n",
    "plt.title('Julia Set (local test)')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Strong Scaling Experiment\n",
    "\n",
    "**Goal:** Measure how execution time decreases as we add more cores to a *fixed-size* problem.\n",
    "\n",
    "**Setup:**\n",
    "- Fixed problem size: **4000 × 4000** pixels\n",
    "- Artificial serial delay: configurable (default 100ms)\n",
    "- Vary thread count: 1, 2, 4, 8, 16, 24, 32, 48\n",
    "- Reserve full node (`--cpus-per-task 48`) to avoid interference from other jobs\n",
    "\n",
    "**Expected behavior:** Speedup will increase with cores but eventually plateau (Amdahl's Law)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the SLURM script with current parameters\n",
    "strong_scaling_script = f'''#!/bin/bash -l\n",
    "\n",
    "# =============================================================================\n",
    "# Strong Scaling Experiment - Julia Set\n",
    "# Fixed problem size ({STRONG_SCALING_SIZE}x{STRONG_SCALING_SIZE}), varying thread count\n",
    "# =============================================================================\n",
    "\n",
    "# Job general details\n",
    "#SBATCH --job-name JuliaSet_StrongScaling\n",
    "#SBATCH --account rfabbret_cours_hpc\n",
    "#SBATCH --mail-type NONE\n",
    "#SBATCH --time 00:29:59\n",
    "\n",
    "# Paths and output\n",
    "#SBATCH --output logs/strong_scaling_%A_%a.out\n",
    "\n",
    "# Resources - reserve full node to avoid interference\n",
    "#SBATCH --nodes 1\n",
    "#SBATCH --ntasks 1\n",
    "#SBATCH --cpus-per-task 48\n",
    "#SBATCH --mem 8G\n",
    "\n",
    "# Node specificities\n",
    "#SBATCH --partition cpu\n",
    "\n",
    "# Array: each task ID is the number of threads to use\n",
    "#SBATCH --array 1,2,4,8,16,24,32,48\n",
    "\n",
    "# Clean environment\n",
    "#SBATCH --export NONE\n",
    "\n",
    "# =============================================================================\n",
    "# Job execution\n",
    "# =============================================================================\n",
    "\n",
    "# Set thread count from array task ID\n",
    "export OMP_NUM_THREADS=${{SLURM_ARRAY_TASK_ID}}\n",
    "\n",
    "# Fixed problem size for strong scaling\n",
    "WIDTH={STRONG_SCALING_SIZE}\n",
    "HEIGHT={STRONG_SCALING_SIZE}\n",
    "\n",
    "# Artificial serial delay (ms)\n",
    "SERIAL_MS={ARTIFICIAL_SERIAL_MS}\n",
    "\n",
    "# Output file\n",
    "OUTPUT_FILE=\"output/strong_${{SLURM_ARRAY_TASK_ID}}cores.bin\"\n",
    "\n",
    "echo \"=== Strong Scaling Experiment ===\"\n",
    "echo \"Job ID: ${{SLURM_JOB_ID}}\"\n",
    "echo \"Array Task ID: ${{SLURM_ARRAY_TASK_ID}}\"\n",
    "echo \"Threads: ${{OMP_NUM_THREADS}}\"\n",
    "echo \"Problem size: ${{WIDTH}} x ${{HEIGHT}}\"\n",
    "echo \"Artificial serial delay: ${{SERIAL_MS}} ms\"\n",
    "echo \"\"\n",
    "\n",
    "# Run the Julia set computation\n",
    "./julia_set ${{WIDTH}} ${{HEIGHT}} ${{OUTPUT_FILE}} ${{SERIAL_MS}}\n",
    "'''\n",
    "\n",
    "with open('job_strong_scaling.sh', 'w') as f:\n",
    "    f.write(strong_scaling_script)\n",
    "\n",
    "print(\"Generated job_strong_scaling.sh\")\n",
    "print(f\"  - Problem size: {STRONG_SCALING_SIZE}x{STRONG_SCALING_SIZE}\")\n",
    "print(f\"  - Artificial serial delay: {ARTIFICIAL_SERIAL_MS} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the Strong Scaling Job\n",
    "\n",
    "Run this command in your terminal:\n",
    "\n",
    "```bash\n",
    "sbatch job_strong_scaling.sh\n",
    "```\n",
    "\n",
    "Monitor your jobs with:\n",
    "```bash\n",
    "squeue -u $USER\n",
    "```\n",
    "\n",
    "Wait for all array tasks to complete before proceeding to the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Weak Scaling Experiment\n",
    "\n",
    "**Goal:** Measure how execution time stays constant as we scale *both* problem size and cores proportionally.\n",
    "\n",
    "**Setup:**\n",
    "- Base problem size: **1000 × 1000** pixels for 1 core\n",
    "- Scale both dimensions by $\\sqrt{N}$ where $N$ is the thread count\n",
    "- This keeps work per thread constant (each thread processes ~1M pixels)\n",
    "\n",
    "| Threads | Width | Height | Total Pixels | Pixels/Thread |\n",
    "|---------|-------|--------|--------------|---------------|\n",
    "| 1 | 1000 | 1000 | 1M | 1M |\n",
    "| 4 | 2000 | 2000 | 4M | 1M |\n",
    "| 16 | 4000 | 4000 | 16M | 1M |\n",
    "| 48 | 6928 | 6928 | ~48M | 1M |\n",
    "\n",
    "**Expected behavior:** Execution time should remain roughly constant (Gustafson's Law)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the SLURM script with current parameters\n",
    "weak_scaling_script = f'''#!/bin/bash -l\n",
    "\n",
    "# =============================================================================\n",
    "# Weak Scaling Experiment - Julia Set\n",
    "# Problem size scales with thread count (constant work per thread)\n",
    "# =============================================================================\n",
    "\n",
    "# Job general details\n",
    "#SBATCH --job-name JuliaSet_WeakScaling\n",
    "#SBATCH --account rfabbret_cours_hpc\n",
    "#SBATCH --mail-type NONE\n",
    "#SBATCH --time 00:29:59\n",
    "\n",
    "# Paths and output\n",
    "#SBATCH --output logs/weak_scaling_%A_%a.out\n",
    "\n",
    "# Resources - reserve full node to avoid interference\n",
    "#SBATCH --nodes 1\n",
    "#SBATCH --ntasks 1\n",
    "#SBATCH --cpus-per-task 48\n",
    "#SBATCH --mem 8G\n",
    "\n",
    "# Node specificities\n",
    "#SBATCH --partition cpu\n",
    "\n",
    "# Array: each task ID is the number of threads to use\n",
    "#SBATCH --array 1,2,4,8,16,24,32,48\n",
    "\n",
    "# Clean environment\n",
    "#SBATCH --export NONE\n",
    "\n",
    "# =============================================================================\n",
    "# Job execution\n",
    "# =============================================================================\n",
    "\n",
    "# Set thread count from array task ID\n",
    "export OMP_NUM_THREADS=${{SLURM_ARRAY_TASK_ID}}\n",
    "\n",
    "# Base size for 1 thread\n",
    "BASE_SIZE={WEAK_SCALING_BASE}\n",
    "\n",
    "# Artificial serial delay (ms)\n",
    "SERIAL_MS={ARTIFICIAL_SERIAL_MS}\n",
    "\n",
    "# Scale dimensions by sqrt(N) to keep work per thread constant\n",
    "# Using bc for floating point math, then converting to integer\n",
    "SCALE=$(echo \"scale=6; sqrt(${{SLURM_ARRAY_TASK_ID}})\" | bc)\n",
    "WIDTH=$(echo \"${{BASE_SIZE}} * ${{SCALE}}\" | bc | cut -d'.' -f1)\n",
    "HEIGHT=${{WIDTH}}  # Keep square\n",
    "\n",
    "# Output file\n",
    "OUTPUT_FILE=\"output/weak_${{SLURM_ARRAY_TASK_ID}}cores.bin\"\n",
    "\n",
    "echo \"=== Weak Scaling Experiment ===\"\n",
    "echo \"Job ID: ${{SLURM_JOB_ID}}\"\n",
    "echo \"Array Task ID: ${{SLURM_ARRAY_TASK_ID}}\"\n",
    "echo \"Threads: ${{OMP_NUM_THREADS}}\"\n",
    "echo \"Scale factor: ${{SCALE}}\"\n",
    "echo \"Problem size: ${{WIDTH}} x ${{HEIGHT}}\"\n",
    "echo \"Artificial serial delay: ${{SERIAL_MS}} ms\"\n",
    "echo \"\"\n",
    "\n",
    "# Run the Julia set computation\n",
    "./julia_set ${{WIDTH}} ${{HEIGHT}} ${{OUTPUT_FILE}} ${{SERIAL_MS}}\n",
    "'''\n",
    "\n",
    "with open('job_weak_scaling.sh', 'w') as f:\n",
    "    f.write(weak_scaling_script)\n",
    "\n",
    "print(\"Generated job_weak_scaling.sh\")\n",
    "print(f\"  - Base size: {WEAK_SCALING_BASE}x{WEAK_SCALING_BASE}\")\n",
    "print(f\"  - Artificial serial delay: {ARTIFICIAL_SERIAL_MS} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the Weak Scaling Job\n",
    "\n",
    "Run this command in your terminal:\n",
    "\n",
    "```bash\n",
    "sbatch job_weak_scaling.sh\n",
    "```\n",
    "\n",
    "Monitor your jobs with:\n",
    "```bash\n",
    "squeue -u $USER\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Results Analysis\n",
    "\n",
    "Once all jobs have completed, run the following cells to parse the output files and generate plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def parse_slurm_output(filename):\n",
    "    \"\"\"Parse a SLURM output file to extract timing information.\"\"\"\n",
    "    result = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "        # Extract key-value pairs\n",
    "        for line in content.split('\\n'):\n",
    "            if ':' in line:\n",
    "                parts = line.split(':', 1)\n",
    "                if len(parts) == 2:\n",
    "                    key = parts[0].strip()\n",
    "                    value = parts[1].strip()\n",
    "                    \n",
    "                    # Try to convert to number\n",
    "                    try:\n",
    "                        if '.' in value:\n",
    "                            result[key] = float(value)\n",
    "                        else:\n",
    "                            result[key] = int(value)\n",
    "                    except ValueError:\n",
    "                        result[key] = value\n",
    "    \n",
    "    return result\n",
    "\n",
    "def collect_results(pattern):\n",
    "    \"\"\"Collect results from all matching SLURM output files.\"\"\"\n",
    "    files = glob.glob(pattern)\n",
    "    results = []\n",
    "    \n",
    "    for f in files:\n",
    "        data = parse_slurm_output(f)\n",
    "        if 'Threads' in data and 'Time' in data:\n",
    "            results.append(data)\n",
    "    \n",
    "    # Sort by thread count\n",
    "    results.sort(key=lambda x: x['Threads'])\n",
    "    return results\n",
    "\n",
    "# Amdahl's law function for curve fitting\n",
    "def amdahl_speedup(N, p):\n",
    "    \"\"\"Amdahl's Law: S(N) = 1 / ((1-p) + p/N)\"\"\"\n",
    "    return 1.0 / ((1.0 - p) + p / N)\n",
    "\n",
    "print(\"Analysis functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect strong scaling results\n",
    "strong_results = collect_results('logs/strong_scaling_*.out')\n",
    "\n",
    "print(\"Strong Scaling Results:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Threads':>10} {'Time (s)':>12} {'Speedup':>10} {'Efficiency':>12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if strong_results:\n",
    "    t1 = strong_results[0]['Time']  # Baseline (1 thread)\n",
    "    \n",
    "    for r in strong_results:\n",
    "        threads = r['Threads']\n",
    "        time = r['Time']\n",
    "        speedup = t1 / time\n",
    "        efficiency = speedup / threads\n",
    "        print(f\"{threads:>10} {time:>12.4f} {speedup:>10.2f} {efficiency:>12.2f}\")\n",
    "else:\n",
    "    print(\"No results found. Have the jobs completed?\")\n",
    "    print(\"Check with: squeue -u $USER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect weak scaling results\n",
    "weak_results = collect_results('logs/weak_scaling_*.out')\n",
    "\n",
    "print(\"Weak Scaling Results:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Threads':>10} {'Size':>12} {'Time (s)':>12} {'Scaled Speedup':>15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if weak_results:\n",
    "    t1 = weak_results[0]['Time']  # Baseline (1 thread)\n",
    "    \n",
    "    for r in weak_results:\n",
    "        threads = r['Threads']\n",
    "        width = r.get('Width', '?')\n",
    "        height = r.get('Height', '?')\n",
    "        time = r['Time']\n",
    "        # For weak scaling, \"scaled speedup\" = N * T1 / TN\n",
    "        # (how much more work we did in the same time)\n",
    "        scaled_speedup = threads * t1 / time\n",
    "        print(f\"{threads:>10} {width}x{height:>6} {time:>12.4f} {scaled_speedup:>15.2f}\")\n",
    "else:\n",
    "    print(\"No results found. Have the jobs completed?\")\n",
    "    print(\"Check with: squeue -u $USER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Scaling Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_strong_scaling(results):\n",
    "    \"\"\"Plot strong scaling results with fitted Amdahl's Law curve.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No strong scaling results to plot.\")\n",
    "        return\n",
    "    \n",
    "    threads = np.array([r['Threads'] for r in results])\n",
    "    times = np.array([r['Time'] for r in results])\n",
    "    t1 = times[0]\n",
    "    speedup = t1 / times\n",
    "    efficiency = speedup / threads\n",
    "    \n",
    "    # Fit Amdahl's Law to the data\n",
    "    try:\n",
    "        popt, pcov = curve_fit(amdahl_speedup, threads, speedup, p0=[0.9], bounds=(0.5, 0.9999))\n",
    "        p_estimated = popt[0]\n",
    "        p_std = np.sqrt(pcov[0, 0])\n",
    "    except:\n",
    "        # Fallback to simple estimation\n",
    "        S_max = speedup[-1]\n",
    "        N_max = threads[-1]\n",
    "        p_estimated = (1 - 1/S_max) / (1 - 1/N_max) if N_max > 1 else 0.95\n",
    "        p_estimated = min(0.999, max(0.5, p_estimated))\n",
    "        p_std = 0\n",
    "    \n",
    "    # Theoretical curves\n",
    "    n_theory = np.linspace(1, max(threads), 100)\n",
    "    ideal = n_theory\n",
    "    amdahl_fit = amdahl_speedup(n_theory, p_estimated)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Speedup plot\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(n_theory, ideal, 'k--', label='Ideal (linear)', linewidth=1.5)\n",
    "    ax1.plot(n_theory, amdahl_fit, 'b-', label=f\"Amdahl fit (p={p_estimated:.4f})\", linewidth=2)\n",
    "    ax1.plot(threads, speedup, 'ro', label='Measured', markersize=10)\n",
    "    ax1.set_xlabel('Number of Threads', fontsize=12)\n",
    "    ax1.set_ylabel('Speedup', fontsize=12)\n",
    "    ax1.set_title('Strong Scaling: Speedup', fontsize=14)\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(0, max(threads) + 2)\n",
    "    ax1.set_ylim(0, max(max(speedup), amdahl_fit[-1]) * 1.1)\n",
    "    \n",
    "    # Efficiency plot\n",
    "    ax2 = axes[1]\n",
    "    ax2.axhline(y=1.0, color='k', linestyle='--', label='Ideal (100%)', linewidth=1.5)\n",
    "    ax2.plot(n_theory, amdahl_fit / n_theory, 'b-', label=f\"Amdahl fit (p={p_estimated:.4f})\", linewidth=2)\n",
    "    ax2.plot(threads, efficiency, 'ro', label='Measured', markersize=10)\n",
    "    ax2.set_xlabel('Number of Threads', fontsize=12)\n",
    "    ax2.set_ylabel('Efficiency (Speedup / Threads)', fontsize=12)\n",
    "    ax2.set_title('Strong Scaling: Efficiency', fontsize=14)\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlim(0, max(threads) + 2)\n",
    "    ax2.set_ylim(0, 1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/strong_scaling_plot.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFitted parallel fraction: p = {p_estimated:.4f}\" + (f\" ± {p_std:.4f}\" if p_std > 0 else \"\"))\n",
    "    print(f\"Serial fraction: (1-p) = {1-p_estimated:.4f}\")\n",
    "    print(f\"Theoretical max speedup (Amdahl): {1/(1-p_estimated):.2f}x\")\n",
    "\n",
    "plot_strong_scaling(strong_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weak_scaling(results):\n",
    "    \"\"\"Plot weak scaling results.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No weak scaling results to plot.\")\n",
    "        return\n",
    "    \n",
    "    threads = np.array([r['Threads'] for r in results])\n",
    "    times = np.array([r['Time'] for r in results])\n",
    "    t1 = times[0]\n",
    "    \n",
    "    # For weak scaling, we want time to stay constant (ideal)\n",
    "    # Scaled speedup = N * T1 / TN (how much more work in same time)\n",
    "    scaled_speedup = threads * t1 / times\n",
    "    \n",
    "    # Weak scaling efficiency\n",
    "    weak_efficiency = t1 / times\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Execution time plot\n",
    "    ax1 = axes[0]\n",
    "    ax1.axhline(y=t1, color='k', linestyle='--', label=f'Ideal ({t1:.2f}s)', linewidth=1.5)\n",
    "    ax1.plot(threads, times, 'go', label='Measured', markersize=10)\n",
    "    # Fit a line to show trend\n",
    "    z = np.polyfit(threads, times, 1)\n",
    "    p_line = np.poly1d(z)\n",
    "    ax1.plot(threads, p_line(threads), 'g-', linewidth=2, alpha=0.7)\n",
    "    ax1.set_xlabel('Number of Threads', fontsize=12)\n",
    "    ax1.set_ylabel('Execution Time (s)', fontsize=12)\n",
    "    ax1.set_title('Weak Scaling: Execution Time', fontsize=14)\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(0, max(threads) + 2)\n",
    "    \n",
    "    # Scaled speedup plot\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(threads, threads, 'k--', label='Ideal (linear)', linewidth=1.5)\n",
    "    ax2.plot(threads, scaled_speedup, 'go', label='Measured', markersize=10)\n",
    "    # Fit a line through origin to show actual scaling\n",
    "    slope = np.sum(threads * scaled_speedup) / np.sum(threads**2)\n",
    "    ax2.plot(threads, slope * threads, 'g-', linewidth=2, alpha=0.7, label=f'Fit (slope={slope:.3f})')\n",
    "    ax2.set_xlabel('Number of Threads', fontsize=12)\n",
    "    ax2.set_ylabel('Scaled Speedup (N × T₁ / Tₙ)', fontsize=12)\n",
    "    ax2.set_title('Weak Scaling: Scaled Speedup', fontsize=14)\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlim(0, max(threads) + 2)\n",
    "    ax2.set_ylim(0, max(threads) * 1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/weak_scaling_plot.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate weak scaling efficiency\n",
    "    print(f\"\\nWeak Scaling Efficiency (T1/TN):\")\n",
    "    for n, eff in zip(threads, weak_efficiency):\n",
    "        print(f\"  {n:2d} threads: {eff:.2f}\")\n",
    "\n",
    "plot_weak_scaling(weak_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Julia Set Visualization\n",
    "\n",
    "Let's visualize one of the computed Julia set images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the largest computed image (48 cores, strong scaling)\n",
    "image_file = 'output/strong_48cores.bin'\n",
    "\n",
    "if os.path.exists(image_file):\n",
    "    img, w, h = read_julia_image(image_file)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    im = ax.imshow(img, cmap='hot')\n",
    "    plt.colorbar(im, ax=ax, label='Iterations', shrink=0.8)\n",
    "    ax.set_title(f'Julia Set (C = -0.8 + 0.156i)\\n{w} × {h} pixels', fontsize=14)\n",
    "    ax.axis('off')\n",
    "    plt.savefig('output/julia_set_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Image file not found: {image_file}\")\n",
    "    print(\"Run the strong scaling experiment first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak Scaling: Resolution Comparison\n",
    "\n",
    "In weak scaling, higher thread counts compute larger images. To visualize the difference in resolution, we crop the **same physical region** (the center of the fractal) from each image. With more threads/pixels, we see finer details in the fractal structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare images from weak scaling by showing the same region\n",
    "weak_files = sorted(glob.glob('output/weak_*cores.bin'), \n",
    "                    key=lambda x: int(re.search(r'weak_(\\d+)cores', x).group(1)))\n",
    "\n",
    "if len(weak_files) >= 4:\n",
    "    # Get the smallest image dimensions (1 core) for the crop size\n",
    "    img_1core, w_base, h_base = read_julia_image(weak_files[0])\n",
    "    crop_size = min(w_base, h_base)  # This is the base resolution (e.g., 1000)\n",
    "    \n",
    "    # Select representative images\n",
    "    selected_indices = [0, 2, 4, -1]  # 1, 4, 16, 48 cores\n",
    "    selected = [weak_files[i] for i in selected_indices if i < len(weak_files)]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    \n",
    "    for ax, f in zip(axes.flat, selected):\n",
    "        if os.path.exists(f):\n",
    "            img, w, h = read_julia_image(f)\n",
    "            cores = int(re.search(r'weak_(\\d+)cores', f).group(1))\n",
    "            \n",
    "            # Crop the center region (same physical area in fractal coordinates)\n",
    "            # The base image shows the full [-1.5, 1.5] x [-1.5, 1.5] domain\n",
    "            # For larger images, we take the center crop_size x crop_size pixels\n",
    "            # which corresponds to a smaller region in fractal space\n",
    "            center_y, center_x = h // 2, w // 2\n",
    "            half_crop = crop_size // 2\n",
    "            crop = img[center_y - half_crop:center_y + half_crop,\n",
    "                       center_x - half_crop:center_x + half_crop]\n",
    "            \n",
    "            ax.imshow(crop, cmap='hot')\n",
    "            ax.set_title(f'{cores} cores: {w}×{h} total\\n(showing {crop_size}×{crop_size} center crop)', fontsize=11)\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Weak Scaling: Same Region, Increasing Resolution\\n'\n",
    "                 'More cores → larger image → finer detail in center crop', \n",
    "                 fontsize=13, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/weak_scaling_images.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nNote: All subplots show a {crop_size}×{crop_size} pixel crop from the center.\")\n",
    "    print(\"Larger simulations (more cores) resolve finer fractal details.\")\n",
    "else:\n",
    "    print(\"Not enough weak scaling images found.\")\n",
    "    print(\"Run the weak scaling experiment first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Discussion Questions\n",
    "\n",
    "After completing the experiments, consider these questions:\n",
    "\n",
    "1. **Strong Scaling:**\n",
    "   - Does the measured speedup match Amdahl's Law prediction?\n",
    "   - What is the fitted parallel fraction of the code?\n",
    "   - At what thread count does adding more cores become inefficient (efficiency < 50%)?\n",
    "   - Try re-running with `ARTIFICIAL_SERIAL_MS = 0`. How does the fitted `p` change?\n",
    "\n",
    "2. **Weak Scaling:**\n",
    "   - Does execution time stay constant as we scale up?\n",
    "   - Why might weak scaling efficiency decrease with more cores?\n",
    "   - Which scaling behavior (strong or weak) is more relevant for your research?\n",
    "\n",
    "3. **Practical Considerations:**\n",
    "   - Why did we reserve the full node even when using fewer cores?\n",
    "   - How would results differ if other jobs were running on the same node?\n",
    "   - What factors besides serial code fraction affect scaling? (hint: memory bandwidth, cache effects)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You have successfully measured strong and weak scaling on an HPC cluster and connected the results to Amdahl's and Gustafson's Laws."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
